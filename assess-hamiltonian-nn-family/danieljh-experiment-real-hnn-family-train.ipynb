{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2334b12a",
   "metadata": {},
   "source": [
    "# Train HNN-family (HNN, D-HNN, DGNet) for 'experiment-real' problem.\n",
    "\n",
    " - Original HNN from hamiltonian_nn by Sam Greydanus\n",
    "\n",
    " - Original D-HNN from dissipative_hnns by Andrew Sosanya\n",
    "\n",
    " - Original DGNet from discrete-autograd by Takashi Matsubara\n",
    "\n",
    " - Modified and adapted by Jae Hoon (Daniel) Lee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7bc426cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import argparse\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "EXPERIMENT_DIR = './experiment-real'\n",
    "sys.path.append(EXPERIMENT_DIR)\n",
    "\n",
    "from data import get_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f0ce7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_jupyter():\n",
    "    return 'ipykernel' in sys.modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6eb4b6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_weights(model, directory, filename):\n",
    "    try:\n",
    "        if not filename.lower().endswith(('.pth', '.pt', '.tar')):\n",
    "            filename += '.pth'\n",
    "        \n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "        \n",
    "        full_path = os.path.join(directory, filename)      \n",
    "        torch.save(model.state_dict(), full_path)\n",
    "        \n",
    "        print(f\"Model's weight has successfully been saved: {full_path}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred while saving the model: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d87f0834",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_weights(model, filepath, device='cpu'):\n",
    "    if not os.path.exists(filepath):\n",
    "        print(f\"Error: file not found: {filepath}\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        state_dict = torch.load(filepath, map_location=torch.device(device))\n",
    "        model.load_state_dict(state_dict)\n",
    "        model.to(device)\n",
    "\n",
    "        print(f\"Model's weight has successfully been loaded: {filepath} (Device: {device})\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred while loading the model: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c7cadf",
   "metadata": {},
   "source": [
    "# DGNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59560055",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0bf2d28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dgnet_args():\n",
    "    parser = argparse.ArgumentParser(description=None)\n",
    "    parser.add_argument('--noretry', dest='noretry', action='store_true', help='not do a finished trial.')\n",
    "    # network, experiments\n",
    "    parser.add_argument('--input_dim', default=2, type=int, help='dimensionality of input tensor')\n",
    "    parser.add_argument('--hidden_dim', default=200, type=int, help='hidden dimension of mlp')\n",
    "    parser.add_argument('--learn_rate', default=1e-3, type=float, help='learning rate')\n",
    "    parser.add_argument('--nonlinearity', default='tanh', type=str, help='neural net nonlinearity')\n",
    "    parser.add_argument('--total_steps', default=2000, type=int, help='number of gradient steps')\n",
    "    # display\n",
    "    parser.add_argument('--print_every', default=200, type=int, help='number of gradient steps between prints')\n",
    "    parser.add_argument('--verbose', dest='verbose', action='store_true', help='verbose?')\n",
    "    parser.add_argument('--name', default='real', type=str, help='only one option right now')\n",
    "    parser.add_argument('--seed', default=0, type=int, help='random seed')\n",
    "    parser.add_argument('--save_dir', default=EXPERIMENT_DIR, type=str, help='where to save the trained model')\n",
    "    # model\n",
    "    parser.add_argument('--model', default='hnn', type=str, help='used model.')\n",
    "    parser.add_argument('--solver', default='dg', type=str, help='used solver.')\n",
    "    parser.add_argument('--friction', default=False, action=\"store_true\", help='use friction parameter')\n",
    "    parser.set_defaults(feature=True)\n",
    "\n",
    "    if is_jupyter():\n",
    "        return parser.parse_args([]) \n",
    "    else:\n",
    "        return parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "77f04bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dgnet_model(device='cpu'):\n",
    "\n",
    "    args = get_dgnet_args()\n",
    "    model = dgnet.DGNet(args.input_dim, args.hidden_dim,\n",
    "                        nonlinearity=args.nonlinearity, friction=args.friction, model=args.model, solver=args.solver)\n",
    "    model = model.to(device)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec558200",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dgnet_train(args):\n",
    "    ''' import from the current directory structure '''\n",
    "    from hamiltonian_nn.utils import L2_loss\n",
    "\n",
    "    device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "    dtype = torch.get_default_dtype()\n",
    "    torch.set_grad_enabled(False)\n",
    "\n",
    "    # set random seed\n",
    "    torch.manual_seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "\n",
    "    # init model and optimizer\n",
    "    model = dgnet.DGNet(args.input_dim, args.hidden_dim,\n",
    "                        nonlinearity=args.nonlinearity, friction=args.friction, model=args.model, solver=args.solver)\n",
    "    model = model.to(device)\n",
    "    optim = torch.optim.Adam(model.parameters(), args.learn_rate, weight_decay=1e-5)\n",
    "\n",
    "    # arrange data\n",
    "    data = get_dataset('pend-real', args.save_dir)\n",
    "    train_x = torch.tensor(data['x'], requires_grad=True, device=device, dtype=dtype)\n",
    "    test_x = torch.tensor(data['test_x'], requires_grad=True, device=device, dtype=dtype)\n",
    "    train_dxdt = torch.tensor(data['dx'], device=device, dtype=dtype)\n",
    "    test_dxdt = torch.tensor(data['test_dx'], device=device, dtype=dtype)\n",
    "\n",
    "    input_dim = train_x.shape[-1]\n",
    "    x1 = train_x[:-1].detach()\n",
    "    x2 = train_x[1:].detach()\n",
    "    dxdt = train_dxdt[:-1].clone()\n",
    "    dt = 1 / 6.\n",
    "\n",
    "    # vanilla train loop\n",
    "    stats = {'train_loss': [], 'test_loss': []}\n",
    "    for step in range(args.total_steps + 1):\n",
    "        with torch.enable_grad():\n",
    "            # train step\n",
    "            dxdt_hat = model.discrete_time_derivative(x1, dt=dt, x2=x2)\n",
    "            loss = L2_loss(dxdt, dxdt_hat)\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "        # run validation\n",
    "        if args.solver == 'implicit':\n",
    "            # because it consumes too long time.\n",
    "            test_loss = torch.tensor(float('nan'))\n",
    "        else:\n",
    "            test_dxdt_hat = model.discrete_time_derivative(test_x, dt=dt)\n",
    "            test_loss = L2_loss(test_dxdt, test_dxdt_hat)\n",
    "\n",
    "        # logging\n",
    "        stats['train_loss'].append(loss.item())\n",
    "        stats['test_loss'].append(test_loss.item())\n",
    "        if args.verbose and step % args.print_every == 0:\n",
    "            print(\"step {}, train_loss {:.4e}, test_loss {:.4e}\".format(step, loss.item(), test_loss.item()))\n",
    "            if args.friction:\n",
    "                print(\"friction g =\", model.g.detach().cpu().numpy())\n",
    "\n",
    "    dxdt_hat = model.discrete_time_derivative(train_x, dt=dt)\n",
    "    train_dist = (train_dxdt - dxdt_hat)**2\n",
    "    test_dxdt_hat = model.discrete_time_derivative(test_x, dt=dt)\n",
    "    test_dist = (test_dxdt - test_dxdt_hat)**2\n",
    "    print('Final train loss {:.4e}\\nFinal test loss {:.4e}'\n",
    "          .format(train_dist.mean().item(), test_dist.mean().item()))\n",
    "    stats['final_train_loss'] = train_dist.mean().item()\n",
    "    stats['final_test_loss'] = test_dist.mean().item()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dfc09b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dgnet_main():\n",
    "    args = get_dgnet_args()\n",
    "\n",
    "    # Explicitly set args.verbose as True\n",
    "    args.verbose = True \n",
    "    args.print_every = 200\n",
    "\n",
    "    # Explicitly set args.friction as True\n",
    "    args.friction = True\n",
    "\n",
    "    # save\n",
    "    os.makedirs(args.save_dir + '/results') if not os.path.exists(args.save_dir + '/results') else None\n",
    "    label = ''\n",
    "    label = label + '-{}-{}'.format(args.model, args.solver)\n",
    "    label = label + '-friction' if args.friction else label\n",
    "    label = label + '-seed{}'.format(args.seed)\n",
    "    name = args.name\n",
    "    result_path = '{}/results/dg-{}{}'.format(args.save_dir, name, label)\n",
    "    path_tar = '{}.tar'.format(result_path)\n",
    "    path_pkl = '{}.pkl'.format(result_path)\n",
    "    path_txt = '{}.txt'.format(result_path)\n",
    "    args.result_path = result_path\n",
    "\n",
    "    if os.path.exists(path_txt):\n",
    "        if args.noretry:\n",
    "            exit()\n",
    "        else:\n",
    "            os.remove(path_txt)\n",
    "\n",
    "    model = dgnet_train(args)\n",
    "    torch.save(model.state_dict(), path_tar)\n",
    "   \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "220cc849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data zip filepath: ./experiment-real/invar_datasets.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/danieljh/py-venv/lib/python3.12/site-packages/torchdiffeq/_impl/misc.py:306: UserWarning: t is not on the same device as y0. Coercing to y0.device.\n",
      "  warnings.warn(\"t is not on the same device as y0. Coercing to y0.device.\")\n",
      "/media/danieljh/shared/Code/piml-comp-git/assess-hamiltonian-nn-family/dgnet/modules.py:36: RuntimeWarning: The iteration is not making good progress, as measured by the \n",
      " improvement from the last ten iterations.\n",
      "  x1 = fsolve(wrapped_func, to_numpy(x0), *args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, train_loss 7.7965e-01, test_loss 9.2601e-01\n",
      "friction g = [0.001]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/danieljh/shared/Code/piml-comp-git/assess-hamiltonian-nn-family/dgnet/modules.py:36: RuntimeWarning: The iteration is not making good progress, as measured by the \n",
      " improvement from the last five Jacobian evaluations.\n",
      "  x1 = fsolve(wrapped_func, to_numpy(x0), *args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 200, train_loss 4.2489e-03, test_loss 7.5466e-01\n",
      "friction g = [0.01766522]\n",
      "step 400, train_loss 2.8716e-03, test_loss 7.6149e-01\n",
      "friction g = [0.01108745]\n",
      "step 600, train_loss 2.3472e-03, test_loss 7.6589e-01\n",
      "friction g = [0.01041034]\n",
      "step 800, train_loss 2.3410e-03, test_loss 7.6676e-01\n",
      "friction g = [0.01081625]\n",
      "step 1000, train_loss 2.2409e-03, test_loss 7.6731e-01\n",
      "friction g = [0.01067518]\n",
      "step 1200, train_loss 2.2785e-03, test_loss 7.6751e-01\n",
      "friction g = [0.01117517]\n",
      "step 1400, train_loss 2.2244e-03, test_loss 7.6768e-01\n",
      "friction g = [0.01092777]\n",
      "step 1600, train_loss 2.2499e-03, test_loss 7.6771e-01\n",
      "friction g = [0.01062583]\n",
      "step 1800, train_loss 2.2260e-03, test_loss 7.6798e-01\n",
      "friction g = [0.0108379]\n",
      "step 2000, train_loss 2.2511e-03, test_loss 7.6758e-01\n",
      "friction g = [0.01055254]\n",
      "Final train loss 1.3461e+00\n",
      "Final test loss 7.6758e-01\n",
      "Model's weight has successfully been saved: ./experiment-real/weights/dgnet_real.pth\n"
     ]
    }
   ],
   "source": [
    "dgnet_model = train_dgnet_main()\n",
    "save_dir = EXPERIMENT_DIR + \"/weights\"\n",
    "save_model_weights(dgnet_model, save_dir,\"dgnet_real\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10adb35",
   "metadata": {},
   "source": [
    "# D-HNN\n",
    "\n",
    "(Dissipative Hamiltonian NN: original)\n",
    "\n",
    "- Original DHNN architecture as implemented by Andrew Sosanya used.\n",
    "\n",
    "- This will not output Hamiltonian itseft but ouput its derivatives only. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301b4716",
   "metadata": {},
   "source": [
    "- To ensure a fair comparison with DGNet, which does not require the mid-point rule, prepared two versions of the dataset:\n",
    "\n",
    "   - 1) The 'as-is' version from the original hamiltonian-nn, which does not apply the mid-point rule.\n",
    "\n",
    "   - 2) The original data preparation version used in dissipative_hnns, which incorporates the mid-point rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4a365f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ObjectView(object):\n",
    "    def __init__(self, d): self.__dict__ = d\n",
    "    \n",
    "def get_dhnn_args(as_dict=False):\n",
    "  arg_dict = {'input_dim': 3,\n",
    "              'hidden_dim': 256, # capacity\n",
    "              'output_dim': 2,\n",
    "              'learning_rate': 1e-2, \n",
    "              'test_every': 100,\n",
    "              'print_every': 200,\n",
    "              'batch_size': 128,\n",
    "              'train_split': 0.80,  # train/test dataset percentage\n",
    "              'total_steps': 5000,  # because we have a synthetic dataset\n",
    "              'device': 'cuda', # {\"cpu\", \"cuda\"} for using GPUs\n",
    "              'seed': 42,\n",
    "              'as_separate': False,\n",
    "              'decay': 0}\n",
    "  return arg_dict if as_dict else ObjectView(arg_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6174f2b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\nIf False, this changes the data preparation keys of the original D-HNN \\nfrom \\'{x,t,dx}_test\\' to \\'test_{x,t,dx}\\', \\nfollowing the keyword ordering convention of HNN\\'s get_dataset().\"\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "use_original_setting = True\n",
    "''' \n",
    "If False, this changes the data preparation keys of the original D-HNN \n",
    "from '{x,t,dx}_test' to 'test_{x,t,dx}', \n",
    "following the keyword ordering convention of HNN's get_dataset().\"\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "68dfe4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dhnn_data(args, save_dir=None):\n",
    "  # adapted to the current directory structures\n",
    "  from dissipative_hnns.utils import read_lipson, str2array\n",
    "\n",
    "  if save_dir is None:\n",
    "    save_dir = './experiment-real/'\n",
    "\n",
    "  dataset_name=\"real_pend_h_1\"\n",
    "  data_str = read_lipson(dataset_name, save_dir)\n",
    "  state, names = str2array(data_str)\n",
    "\n",
    "  # estimate dx using finite differences\n",
    "  if use_original_setting:\n",
    "    # prep at the original d_hnn\n",
    "    data = {k: state[:,i:i+1] for i, k in enumerate(names)}\n",
    "    x = state[:,2:4]\n",
    "    dx = (x[1:] - x[:-1]) / (data['t'][1:] - data['t'][:-1])\n",
    "    dx[:-1] = (dx[:-1] + dx[1:]) / 2  # midpoint rule\n",
    "    x, t = x[1:], data['t'][1:]\n",
    "\n",
    "  else:\n",
    "    # Modify D-HNN data prep to align with HNN standards.\n",
    "    data = {k: state[:,i:i+1] for i, k in enumerate(names)}\n",
    "    print('data-prep-step-0 =>  data.keys:', data.keys())\n",
    "\n",
    "    data['x'] = state[:,2:4]\n",
    "    data['dx'] = (data['x'][1:] - data['x'][:-1]) / (data['t'][1:] - data['t'][:-1])\n",
    "    data['x'] = data['x'][:-1]\n",
    "    data['t'] = data['t'][:-1]\n",
    "    x, t = data['x'], data['t']\n",
    "    print('data-prep-step-1 =>  data.keys:', data.keys())#'''\n",
    "  \n",
    "  if use_original_setting:\n",
    "    # prep at the original d_hnn\n",
    "    split_ix = int(state.shape[0] * args.train_split) # train / test split\n",
    "\n",
    "    # \"Align D-HNN keyword naming with HNN's get_dataset() convention.\n",
    "    data['x'], data['test_x'] = x[:split_ix], x[split_ix:]\n",
    "    data['t'], data['test_t'] = 0*x[:split_ix,...,:1], 0*x[split_ix:,...,:1] # H = not time varying\n",
    "    data['dx'], data['test_dx'] = dx[:split_ix], dx[split_ix:]\n",
    "    data['time'], data['test_time'] = t[:split_ix], t[split_ix:]\n",
    "\n",
    "    for key in data.keys():\n",
    "       print(f\"data[{key}]: {data[key].shape}\")\n",
    "\n",
    "    return data\n",
    "  else:\n",
    "    # Modify D-HNN data prep to be compatible with HNN and DGNet.\n",
    "    data['t'] = 0 * t\n",
    "    data['time'] = t\n",
    "\n",
    "    train_set_size = int(len(data['x']) * args.train_split)\n",
    "    test_set_size = int(len(data['x']) * (1-args.train_split))\n",
    "    test_start_ix = train_set_size\n",
    "    a = test_start_ix # plays the same role as split_ix\n",
    "    b = test_start_ix + test_set_size\n",
    "\n",
    "    print('data-prep-step-2 =>  data.keys:', data.keys())\n",
    "  \n",
    "    split_data = {}\n",
    "    for k, v in data.items():\n",
    "      split_data[k] = np.concatenate([v[:a],v[b:]], axis=0) # train/test\n",
    "      split_data['test_' + k] = v[a:b]\n",
    " \n",
    "\n",
    "    data = split_data\n",
    "    print('data-prep-step-end =>  data.keys:', data.keys())\n",
    "    for key in data.keys():\n",
    "      print(f\"data[{key}]: {data[key].shape}\")\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3a5d8558",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data[trial]: (556, 1)\n",
      "data[t]: (444, 1)\n",
      "data[o]: (556, 1)\n",
      "data[v]: (556, 1)\n",
      "data[do]: (556, 0)\n",
      "data[dv]: (556, 0)\n",
      "data[x]: (444, 2)\n",
      "data[test_x]: (111, 2)\n",
      "data[test_t]: (111, 1)\n",
      "data[dx]: (444, 2)\n",
      "data[test_dx]: (111, 2)\n",
      "data[time]: (444, 1)\n",
      "data[test_time]: (111, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:8: SyntaxWarning: invalid escape sequence '\\d'\n",
      "<>:8: SyntaxWarning: invalid escape sequence '\\d'\n",
      "/tmp/ipykernel_18209/4241583307.py:8: SyntaxWarning: invalid escape sequence '\\d'\n",
      "  plt.xlabel('$\\\\theta$') ; plt.ylabel('$\\dot \\\\theta$')\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 280x320 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "args = get_dhnn_args()\n",
    "data = get_dhnn_data(args=args)\n",
    "\n",
    "plt.figure(figsize=[2.8,3.2], dpi=100)\n",
    "plt.plot(data['x'][:,0], data['x'][:,1], '.', label='Training data')\n",
    "plt.plot(data['test_x'][:,0], data['test_x'][:,1], '.', label='Test data')\n",
    "plt.title('Real pendulum data')\n",
    "plt.xlabel('$\\\\theta$') ; plt.ylabel('$\\dot \\\\theta$')\n",
    "plt.legend(fontsize=8, loc='upper right') ; plt.xlim(-1.6,1.6) ; plt.ylim(-1.6,2.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3b0918c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copied form dissipative_hnns, then modified\n",
    "import time\n",
    "\n",
    "\n",
    "def get_batch(v, step, args):  # helper function for moving batches of data to/from GPU\n",
    "  dataset_size, num_features = v.shape\n",
    "  bix = (step*args.batch_size) % dataset_size\n",
    "  v_batch = v[bix:bix + args.batch_size, :]  # select next batch\n",
    "  return torch.tensor(v_batch, requires_grad=True,  dtype=torch.float32, device=args.device)\n",
    "\n",
    "\n",
    "def dhnn_train(model, args, data):\n",
    "  \"\"\" General training function\"\"\"\n",
    "  model = model.to(args.device)  # put the model on the GPU\n",
    "  optimizer = torch.optim.Adam(model.parameters(), lr=args.learning_rate, weight_decay=args.decay)  # setting the Optimizer\n",
    "\n",
    "  model.train()     # doesn't make a difference for now\n",
    "  t0 = time.time()  # logging the time\n",
    "  results = {'train_loss':[], 'test_loss':[], 'test_acc':[], 'global_step':0}  # Logging the results\n",
    "\n",
    "  for step in range(args.total_steps):  # training loop \n",
    "\n",
    "    if use_original_setting:\n",
    "      # original setting at dhnn\n",
    "      x, t, dx = [get_batch(data[k], step, args) for k in ['x', 't', 'dx']]\n",
    "\n",
    "    else:\n",
    "      # modified setting\n",
    "      x = torch.tensor(data['x'], requires_grad=True,  dtype=torch.float32, device=args.device)\n",
    "      t = torch.tensor(data['t'], requires_grad=True,  dtype=torch.float32, device=args.device)\n",
    "      dx = torch.tensor(data['dx'], requires_grad=True,  dtype=torch.float32, device=args.device)#'''\n",
    "    \n",
    "    dx_hat = model(x, t=t)  # feeding forward\n",
    "    loss = (dx-dx_hat).pow(2).mean()  # L2 loss function\n",
    "    loss.backward(retain_graph=False); optimizer.step(); optimizer.zero_grad()  # backpropogation\n",
    "\n",
    "    results['train_loss'].append(loss.item())  # logging the training loss\n",
    "\n",
    "    # Testing our data with desired frequency (test_every)\n",
    "    if step % args.test_every == 0:\n",
    "      # modified setting\n",
    "      test_x, test_t, test_dx = data['test_x'], data['test_t'], data['test_dx']\n",
    "      test_x = torch.tensor(data['test_x'], requires_grad=True,  dtype=torch.float32, device=args.device)\n",
    "      test_t = torch.tensor(data['test_t'], requires_grad=True,  dtype=torch.float32, device=args.device)\n",
    "      test_dx = torch.tensor(data['test_dx'], requires_grad=True,  dtype=torch.float32, device=args.device)\n",
    "      test_dx_hat = model(test_x, t=test_t)\n",
    "      test_loss = (test_dx-test_dx_hat).pow(2).mean().item()\n",
    "\n",
    "    results['test_loss'].append(test_loss)\n",
    "    if step % args.print_every == 0:\n",
    "      print('step {}, dt {:.3f}, train_loss {:.2e}, test_loss {:.2e}'\n",
    "            .format(step, time.time()-t0, loss.item(), test_loss)) #.item() is just the integer of PyTorch scalar. \n",
    "      t0 = time.time()\n",
    "  model = model.cpu()\n",
    "  return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a1a446d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dhnn_model(input_dim, hidden_dim, device='cpu'):\n",
    "    # Use Andrew Sosanya's implementation as-is.\n",
    "    from dissipative_hnns.models import DHNN\n",
    "\n",
    "    model = DHNN(input_dim, hidden_dim) \n",
    "    model.to(device) \n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "05304625",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dhnn_main():\n",
    "    torch.set_grad_enabled(True)\n",
    "\n",
    "    args = get_dhnn_args()\n",
    "    args.batch_size = 15\n",
    "    args.total_steps = 20000\n",
    "    args.learning_rate = 5e-4\n",
    "    args.test_every = 500\n",
    "    args.device = 'cuda'\n",
    "    [f(args.seed) for f in [np.random.seed, torch.manual_seed, torch.cuda.manual_seed_all]]\n",
    "    data = get_dhnn_data(args=args)\n",
    "    model = get_dhnn_model(args.input_dim, args.hidden_dim, args.device)\n",
    "    stats = dhnn_train(model, args, data)  # training the model\n",
    "    return model, stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7532127e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data[trial]: (556, 1)\n",
      "data[t]: (444, 1)\n",
      "data[o]: (556, 1)\n",
      "data[v]: (556, 1)\n",
      "data[do]: (556, 0)\n",
      "data[dv]: (556, 0)\n",
      "data[x]: (444, 2)\n",
      "data[test_x]: (111, 2)\n",
      "data[test_t]: (111, 1)\n",
      "data[dx]: (444, 2)\n",
      "data[test_dx]: (111, 2)\n",
      "data[time]: (444, 1)\n",
      "data[test_time]: (111, 1)\n",
      "step 0, dt 0.011, train_loss 1.17e+00, test_loss 5.68e-01\n",
      "step 200, dt 0.508, train_loss 9.00e-03, test_loss 5.68e-01\n",
      "step 400, dt 0.490, train_loss 2.13e-03, test_loss 5.68e-01\n",
      "step 600, dt 0.502, train_loss 8.80e-04, test_loss 2.59e-03\n",
      "step 800, dt 0.881, train_loss 9.81e-03, test_loss 2.59e-03\n",
      "step 1000, dt 0.903, train_loss 1.06e-03, test_loss 1.56e-03\n",
      "step 1200, dt 0.853, train_loss 1.73e-03, test_loss 1.56e-03\n",
      "step 1400, dt 0.504, train_loss 4.00e-03, test_loss 1.56e-03\n",
      "step 1600, dt 0.491, train_loss 7.50e-03, test_loss 2.16e-03\n",
      "step 1800, dt 0.351, train_loss 8.71e-04, test_loss 2.16e-03\n",
      "step 2000, dt 0.361, train_loss 1.84e-03, test_loss 8.54e-04\n",
      "step 2200, dt 0.357, train_loss 3.08e-03, test_loss 8.54e-04\n",
      "step 2400, dt 0.359, train_loss 1.60e-03, test_loss 8.54e-04\n",
      "step 2600, dt 0.412, train_loss 6.10e-04, test_loss 1.21e-03\n",
      "step 2800, dt 0.410, train_loss 8.05e-04, test_loss 1.21e-03\n",
      "step 3000, dt 0.416, train_loss 2.31e-03, test_loss 1.97e-03\n",
      "step 3200, dt 0.394, train_loss 2.32e-03, test_loss 1.97e-03\n",
      "step 3400, dt 0.405, train_loss 7.95e-04, test_loss 1.97e-03\n",
      "step 3600, dt 0.408, train_loss 9.33e-04, test_loss 6.26e-03\n",
      "step 3800, dt 0.394, train_loss 1.94e-03, test_loss 6.26e-03\n",
      "step 4000, dt 0.376, train_loss 1.70e-03, test_loss 3.33e-03\n",
      "step 4200, dt 0.409, train_loss 3.87e-04, test_loss 3.33e-03\n",
      "step 4400, dt 0.412, train_loss 8.19e-04, test_loss 3.33e-03\n",
      "step 4600, dt 0.341, train_loss 7.51e-04, test_loss 1.54e-03\n",
      "step 4800, dt 0.350, train_loss 1.53e-03, test_loss 1.54e-03\n",
      "step 5000, dt 0.355, train_loss 4.49e-04, test_loss 8.48e-04\n",
      "step 5200, dt 0.346, train_loss 1.02e-03, test_loss 8.48e-04\n",
      "step 5400, dt 0.353, train_loss 9.47e-04, test_loss 8.48e-04\n",
      "step 5600, dt 0.351, train_loss 1.15e-03, test_loss 1.65e-03\n",
      "step 5800, dt 0.339, train_loss 5.26e-04, test_loss 1.65e-03\n",
      "step 6000, dt 0.351, train_loss 5.97e-04, test_loss 8.30e-04\n",
      "step 6200, dt 0.354, train_loss 5.97e-04, test_loss 8.30e-04\n",
      "step 6400, dt 0.357, train_loss 1.14e-03, test_loss 8.30e-04\n",
      "step 6600, dt 0.362, train_loss 4.24e-04, test_loss 1.22e-03\n",
      "step 6800, dt 0.353, train_loss 6.71e-04, test_loss 1.22e-03\n",
      "step 7000, dt 0.361, train_loss 1.37e-03, test_loss 1.16e-03\n",
      "step 7200, dt 0.350, train_loss 2.64e-03, test_loss 1.16e-03\n",
      "step 7400, dt 0.476, train_loss 2.17e-03, test_loss 1.16e-03\n",
      "step 7600, dt 0.490, train_loss 5.35e-04, test_loss 1.95e-03\n",
      "step 7800, dt 0.494, train_loss 7.92e-04, test_loss 1.95e-03\n",
      "step 8000, dt 0.492, train_loss 6.85e-04, test_loss 2.00e-03\n",
      "step 8200, dt 0.483, train_loss 1.05e-03, test_loss 2.00e-03\n",
      "step 8400, dt 0.518, train_loss 6.11e-04, test_loss 2.00e-03\n",
      "step 8600, dt 0.510, train_loss 1.47e-03, test_loss 1.33e-03\n",
      "step 8800, dt 0.443, train_loss 2.14e-03, test_loss 1.33e-03\n",
      "step 9000, dt 0.378, train_loss 2.24e-03, test_loss 1.91e-03\n",
      "step 9200, dt 0.487, train_loss 3.97e-04, test_loss 1.91e-03\n",
      "step 9400, dt 0.477, train_loss 8.81e-04, test_loss 1.91e-03\n",
      "step 9600, dt 0.484, train_loss 1.10e-03, test_loss 7.92e-04\n",
      "step 9800, dt 0.477, train_loss 1.66e-03, test_loss 7.92e-04\n",
      "step 10000, dt 0.493, train_loss 5.46e-04, test_loss 9.97e-04\n",
      "step 10200, dt 0.509, train_loss 6.11e-04, test_loss 9.97e-04\n",
      "step 10400, dt 0.501, train_loss 9.66e-04, test_loss 9.97e-04\n",
      "step 10600, dt 0.507, train_loss 1.78e-03, test_loss 7.91e-04\n",
      "step 10800, dt 0.892, train_loss 7.55e-04, test_loss 7.91e-04\n",
      "step 11000, dt 0.908, train_loss 7.40e-04, test_loss 1.04e-03\n",
      "step 11200, dt 0.917, train_loss 9.24e-04, test_loss 1.04e-03\n",
      "step 11400, dt 0.909, train_loss 1.22e-03, test_loss 1.04e-03\n",
      "step 11600, dt 0.899, train_loss 3.90e-04, test_loss 1.03e-03\n",
      "step 11800, dt 0.906, train_loss 5.28e-04, test_loss 1.03e-03\n",
      "step 12000, dt 0.924, train_loss 5.50e-04, test_loss 1.67e-03\n",
      "step 12200, dt 0.688, train_loss 7.38e-04, test_loss 1.67e-03\n",
      "step 12400, dt 0.521, train_loss 4.96e-04, test_loss 1.67e-03\n",
      "step 12600, dt 0.494, train_loss 8.50e-04, test_loss 1.66e-03\n",
      "step 12800, dt 0.488, train_loss 8.43e-04, test_loss 1.66e-03\n",
      "step 13000, dt 0.492, train_loss 8.19e-04, test_loss 1.16e-03\n",
      "step 13200, dt 0.508, train_loss 2.71e-04, test_loss 1.16e-03\n",
      "step 13400, dt 0.512, train_loss 4.65e-04, test_loss 1.16e-03\n",
      "step 13600, dt 0.493, train_loss 5.68e-04, test_loss 1.96e-03\n",
      "step 13800, dt 0.494, train_loss 9.74e-04, test_loss 1.96e-03\n",
      "step 14000, dt 0.487, train_loss 3.26e-04, test_loss 7.76e-04\n",
      "step 14200, dt 0.484, train_loss 3.72e-04, test_loss 7.76e-04\n",
      "step 14400, dt 0.477, train_loss 9.33e-04, test_loss 7.76e-04\n",
      "step 14600, dt 0.476, train_loss 9.64e-04, test_loss 9.46e-04\n",
      "step 14800, dt 0.507, train_loss 7.66e-04, test_loss 9.46e-04\n",
      "step 15000, dt 0.494, train_loss 3.06e-04, test_loss 8.69e-04\n",
      "step 15200, dt 0.475, train_loss 9.72e-04, test_loss 8.69e-04\n",
      "step 15400, dt 0.480, train_loss 7.44e-04, test_loss 8.69e-04\n",
      "step 15600, dt 0.492, train_loss 1.03e-03, test_loss 9.90e-04\n",
      "step 15800, dt 0.474, train_loss 5.69e-04, test_loss 9.90e-04\n",
      "step 16000, dt 0.487, train_loss 6.89e-04, test_loss 1.27e-03\n",
      "step 16200, dt 0.476, train_loss 1.56e-03, test_loss 1.27e-03\n",
      "step 16400, dt 0.500, train_loss 8.22e-04, test_loss 1.27e-03\n",
      "step 16600, dt 0.507, train_loss 4.36e-04, test_loss 1.43e-03\n",
      "step 16800, dt 0.518, train_loss 8.61e-04, test_loss 1.43e-03\n",
      "step 17000, dt 0.522, train_loss 8.47e-04, test_loss 1.55e-03\n",
      "step 17200, dt 0.475, train_loss 6.86e-04, test_loss 1.55e-03\n",
      "step 17400, dt 0.408, train_loss 3.88e-04, test_loss 1.55e-03\n",
      "step 17600, dt 0.410, train_loss 8.09e-04, test_loss 1.14e-03\n",
      "step 17800, dt 0.416, train_loss 6.46e-04, test_loss 1.14e-03\n",
      "step 18000, dt 0.406, train_loss 6.69e-04, test_loss 1.29e-03\n",
      "step 18200, dt 0.403, train_loss 5.73e-04, test_loss 1.29e-03\n",
      "step 18400, dt 0.405, train_loss 6.62e-04, test_loss 1.29e-03\n",
      "step 18600, dt 0.413, train_loss 9.71e-04, test_loss 8.50e-04\n",
      "step 18800, dt 0.397, train_loss 1.16e-03, test_loss 8.50e-04\n",
      "step 19000, dt 0.404, train_loss 3.83e-04, test_loss 9.48e-04\n",
      "step 19200, dt 0.398, train_loss 8.17e-04, test_loss 9.48e-04\n",
      "step 19400, dt 0.435, train_loss 7.52e-04, test_loss 9.48e-04\n",
      "step 19600, dt 0.487, train_loss 8.33e-04, test_loss 1.10e-03\n",
      "step 19800, dt 0.482, train_loss 2.66e-04, test_loss 1.10e-03\n",
      "Model's weight has successfully been saved: ./experiment-real/weights/dhnn_real_with_midpts.pth\n"
     ]
    }
   ],
   "source": [
    "dhnn_model, dhnn_stats = train_dhnn_main()\n",
    "\n",
    "save_dir = EXPERIMENT_DIR + \"/weights\"\n",
    "save_model_weights(dhnn_model, save_dir,\"dhnn_real_with_midpts\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8949eb3a",
   "metadata": {},
   "source": [
    "# D-HNN2\n",
    "\n",
    "(Dissipative Hamiltonian NN: modified)\n",
    "\n",
    "- This modification is similar to HNN architecture of Sam Greydanus\n",
    "\n",
    "  - 1) This modified architecture now outputs the Hamiltonian.\n",
    "\n",
    "  - 2) Instead, time-derivative() method is added, just like the original HNN of Sam Greydanus.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b40c809a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Modification of DHNN to DHNN2 architecture \n",
    "based on Sam Greydanus' original HNN implementation\n",
    "\"\"\"\n",
    "class MLP2(torch.nn.Module): \n",
    "    def __init__(self, input_dim, output_dim, hidden_dim):\n",
    "      super(MLP2, self).__init__()\n",
    "      self.lin_1 = nn.Linear(input_dim, hidden_dim)\n",
    "      self.lin_2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "      self.lin_3 = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x, t=None):\n",
    "      inputs = torch.cat([x, t], axis=-1) if t is not None else x\n",
    "      h = self.lin_1(inputs).tanh() \n",
    "      h = h + self.lin_2(h).tanh()\n",
    "      y_hat = self.lin_3(h)\n",
    "      return y_hat\n",
    "\n",
    "class DHNN2(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(DHNN2, self).__init__()\n",
    "        # Since H and D are scalar, we set output_dim=1.\n",
    "        self.mlp_h = MLP2(input_dim, 1, hidden_dim) \n",
    "        self.mlp_d = MLP2(input_dim, 1, hidden_dim) \n",
    "        # Levi-Civita permutation tensor M (HNN style)\n",
    "        self.register_buffer('M', self.permutation_tensor(2))\n",
    "\n",
    "    def forward(self, x, t=None): \n",
    "        \"\"\"\n",
    "        Inputs x and t, returns the scalar Hamiltonian (H) and Dissipation (D) functions.\n",
    "        \"\"\"\n",
    "        inputs = torch.cat([x, t], axis=-1) if t is not None else x\n",
    "        D = self.mlp_d(inputs)\n",
    "        H = self.mlp_h(inputs)\n",
    "        return H, D\n",
    "\n",
    "    def time_derivative(self, x, t=None, as_separate=False):\n",
    "        \"\"\"\n",
    "        Calculates the vector field (dx/dt) using gradients of H and D.\n",
    "        \"\"\"\n",
    "        # Ensures gradient tracking, just like HNN.\n",
    "        # x tensor should already have requires_grad=True!\n",
    "        assert x.requires_grad, \"Input tensor x must require grad for time_derivative calculation.\"\n",
    "\n",
    "        H, D = self.forward(x, t) # output: H and D (scalar)\n",
    "\n",
    "        # Gradient of H (Symplectic gradient)\n",
    "        dHdx = torch.autograd.grad(H.sum(), x, create_graph=True)[0]\n",
    "        # Computes the rotational component of the J * dHdx form (J: symplectic matrix)\n",
    "        rot_component = dHdx @ self.M.t()\n",
    "\n",
    "        # Gradient of D (Irrotational gradient)\n",
    "        dDdx = torch.autograd.grad(D.sum(), x, create_graph=True)[0]\n",
    "        irr_component = dDdx\n",
    "\n",
    "        if as_separate:\n",
    "            return irr_component, rot_component\n",
    "\n",
    "        return irr_component + rot_component\n",
    "\n",
    "    def permutation_tensor(self, n):\n",
    "        # permutation_tensor implementation of the original HNN code\n",
    "        M = torch.eye(n)\n",
    "        # assuming canonical coordinates [q, p]\n",
    "        M = torch.cat([M[n//2:], -M[:n//2]])\n",
    "        return M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "35e04ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simplifies accessing the hyperparameters.\n",
    "class ObjectView(object):\n",
    "    def __init__(self, d): self.__dict__ = d\n",
    "    \n",
    "def get_dhnn2_args(as_dict=False):\n",
    "  arg_dict = {'input_dim': 3,\n",
    "              'hidden_dim': 256, # capacity\n",
    "              'output_dim': 2,\n",
    "              'learning_rate': 1e-2, \n",
    "              'test_every': 100,\n",
    "              'print_every': 200,\n",
    "              'batch_size': 128,\n",
    "              'train_split': 0.80,  # train/test dataset percentage\n",
    "              'total_steps': 5000,  # because we have a synthetic dataset\n",
    "              'device': 'cuda', # {\"cpu\", \"cuda\"} for using GPUs\n",
    "              'seed': 42,\n",
    "              'as_separate': False,\n",
    "              'decay': 0,\n",
    "              'verbose': True}\n",
    "  return arg_dict if as_dict else ObjectView(arg_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0c77960f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dhnn2_model(device='cpu'):\n",
    "    args = get_dhnn2_args()    \n",
    "    model = DHNN2(args.input_dim, args.hidden_dim) \n",
    "    model.to(device) \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "048d6180",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch2(v, step, args):  # helper function for moving batches of data to/from GPU\n",
    "  dataset_size, num_features = v.shape\n",
    "  bix = (step*args.batch_size) % dataset_size\n",
    "  v_batch = v[bix:bix + args.batch_size, :]  # select next batch\n",
    "  return torch.tensor(v_batch, requires_grad=True,  dtype=torch.float32, device=args.device)\n",
    "\n",
    "\n",
    "def dhnn2_train():\n",
    "    # adapted to the current directory structures.\n",
    "    from hamiltonian_nn.utils import L2_loss\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model = get_dhnn2_model(device)\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    # Check if model's parameters is updatable\n",
    "    for param in model.parameters():\n",
    "       assert param.requires_grad, \"Model parameters must require grad for training.\"\n",
    "\n",
    "    args = get_dhnn2_args()\n",
    "\n",
    "    # set random seed\n",
    "    torch.manual_seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "\n",
    "    # Adam optimizer\n",
    "    optim = torch.optim.Adam(model.parameters(), args.learning_rate, weight_decay=1e-5)\n",
    "\n",
    "    # arrange data (dataset loading)\n",
    "    args = get_dhnn2_args()\n",
    "    data = get_dhnn_data(args)\n",
    "    \n",
    "    x = torch.tensor( data['x'], requires_grad=True, dtype=torch.float32, device=device)\n",
    "    t = torch.tensor( data['t'], requires_grad=True, dtype=torch.float32, device=device)\n",
    "    dx = torch.tensor(data['dx'], dtype=torch.float32, device=device) \n",
    "\n",
    "    test_x = torch.tensor( data['test_x'], requires_grad=True, dtype=torch.float32, device=device)\n",
    "    test_t = torch.tensor( data['test_t'], requires_grad=True, dtype=torch.float32, device=device)\n",
    "    test_dx = torch.tensor(data['test_dx'], dtype=torch.float32, device=device) \n",
    "\n",
    "    # vanilla train loop\n",
    "    stats = {'train_loss': [], 'test_loss': []}\n",
    "    for step in range(args.total_steps+1):\n",
    "\n",
    "        # train step\n",
    "\n",
    "        # Check the 'requires_grad' property of x tensor\n",
    "        assert x.requires_grad, \"Input tensor x must require grad for training.\" \n",
    "\n",
    "        # Use DHNN2's time_derivative() method (instead of DNN's forward())\n",
    "        dx_hat = model.time_derivative(x, t=t) # We have to pass 't' also as an argument!\n",
    "        loss = L2_loss(dx, dx_hat)\n",
    "        loss.backward() ; optim.step() ; optim.zero_grad()\n",
    "\n",
    "        # run validation (use with torch.no_grad())\n",
    "        test_dx_hat = model.time_derivative(test_x, test_t) # We have to pass 'test_t' also as an argument!\n",
    "        test_loss = L2_loss(test_dx, test_dx_hat)\n",
    "\n",
    "        # logging\n",
    "        stats['train_loss'].append(loss.item())\n",
    "        stats['test_loss'].append(test_loss.item())\n",
    "        if args.verbose and step % args.print_every == 0:\n",
    "            print(\"step {}, train_loss {:.4e}, test_loss {:.4e}\".format(step, loss.item(), test_loss.item()))\n",
    "\n",
    "    # Final evaluation\n",
    "    train_dx_hat = model.time_derivative(x,t) # We should pass 't' also as an argument!\n",
    "    train_dist = (dx - train_dx_hat)**2\n",
    "    test_dx_hat = model.time_derivative(test_x, test_t) # We should pass 't' also as an argument!\n",
    "    test_dist = (test_dx - test_dx_hat)**2\n",
    "        \n",
    "    print('Final train loss {:.4e} +/- {:.4e}\\nFinal test loss {:.4e} +/- {:.4e}'\n",
    "          .format(train_dist.mean().item(), train_dist.std().item()/np.sqrt(train_dist.shape[0]),\n",
    "                  test_dist.mean().item(), test_dist.std().item()/np.sqrt(test_dist.shape[0])))\n",
    "\n",
    "    # After trainin finished, we switch the global grad tracking state back to the True (default)\n",
    "    torch.set_grad_enabled(True) \n",
    "    \n",
    "    return model, stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9e354660",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data[trial]: (556, 1)\n",
      "data[t]: (444, 1)\n",
      "data[o]: (556, 1)\n",
      "data[v]: (556, 1)\n",
      "data[do]: (556, 0)\n",
      "data[dv]: (556, 0)\n",
      "data[x]: (444, 2)\n",
      "data[test_x]: (111, 2)\n",
      "data[test_t]: (111, 1)\n",
      "data[dx]: (444, 2)\n",
      "data[test_dx]: (111, 2)\n",
      "data[time]: (444, 1)\n",
      "data[test_time]: (111, 1)\n",
      "step 0, train_loss 9.0422e-01, test_loss 2.9717e+01\n",
      "step 200, train_loss 5.5624e-04, test_loss 1.7292e-03\n",
      "step 400, train_loss 4.9889e-04, test_loss 1.1962e-03\n",
      "step 600, train_loss 4.8536e-04, test_loss 9.9881e-04\n",
      "step 800, train_loss 4.8124e-04, test_loss 8.9049e-04\n",
      "step 1000, train_loss 4.8107e-04, test_loss 8.2493e-04\n",
      "step 1200, train_loss 5.2870e-04, test_loss 8.3898e-04\n",
      "step 1400, train_loss 4.8577e-04, test_loss 7.6563e-04\n",
      "step 1600, train_loss 4.8687e-04, test_loss 7.4845e-04\n",
      "step 1800, train_loss 4.8971e-04, test_loss 7.5382e-04\n",
      "step 2000, train_loss 4.9649e-04, test_loss 7.4196e-04\n",
      "step 2200, train_loss 4.9264e-04, test_loss 7.3067e-04\n",
      "step 2400, train_loss 4.9629e-04, test_loss 7.5195e-04\n",
      "step 2600, train_loss 4.9861e-04, test_loss 7.2872e-04\n",
      "step 2800, train_loss 5.0137e-04, test_loss 7.3015e-04\n",
      "step 3000, train_loss 5.0452e-04, test_loss 7.2356e-04\n",
      "step 3200, train_loss 5.0829e-04, test_loss 7.3497e-04\n",
      "step 3400, train_loss 5.1105e-04, test_loss 7.5517e-04\n",
      "step 3600, train_loss 5.1536e-04, test_loss 7.6737e-04\n",
      "step 3800, train_loss 5.1647e-04, test_loss 7.7528e-04\n",
      "step 4000, train_loss 5.2500e-04, test_loss 7.5725e-04\n",
      "step 4200, train_loss 6.3079e-04, test_loss 1.0729e-03\n",
      "step 4400, train_loss 5.2334e-04, test_loss 7.9285e-04\n",
      "step 4600, train_loss 5.2710e-04, test_loss 7.9166e-04\n",
      "step 4800, train_loss 5.2976e-04, test_loss 7.9689e-04\n",
      "step 5000, train_loss 5.5630e-04, test_loss 7.9914e-04\n",
      "Final train loss 5.3456e-04 +/- 3.8771e-05\n",
      "Final test loss 7.9914e-04 +/- 1.9047e-04\n",
      "Model's weight has successfully been saved: ./experiment-real/weights/dhnn2_real.pth\n"
     ]
    }
   ],
   "source": [
    "dhnn2_model, dhhn2_stats = dhnn2_train()\n",
    "save_dir = EXPERIMENT_DIR + \"/weights\"\n",
    "save_model_weights(dhnn2_model, save_dir,\"dhnn2_real\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52834910",
   "metadata": {},
   "source": [
    "# HNN2\n",
    "\n",
    "(Hamiltonian NN modified for GPU support)\n",
    "\n",
    "- The original HNN written by Sam Greydaus is not GPU-trainable\n",
    "\n",
    "- DGNet, D-HNN are trained on the same torch GPU context, so HNN also needs to be GPU-trainable\n",
    "\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f82b7412",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hnn2_args():\n",
    "    parser = argparse.ArgumentParser(description=None)\n",
    "    parser.add_argument('--input_dim', default=2, type=int, help='dimensionality of input tensor')\n",
    "    parser.add_argument('--hidden_dim', default=200, type=int, help='hidden dimension of mlp')\n",
    "    parser.add_argument('--learn_rate', default=1e-3, type=float, help='learning rate')\n",
    "    parser.add_argument('--nonlinearity', default='tanh', type=str, help='neural net nonlinearity')\n",
    "    parser.add_argument('--total_steps', default=2000, type=int, help='number of gradient steps')\n",
    "    parser.add_argument('--print_every', default=200, type=int, help='number of gradient steps between prints')\n",
    "    parser.add_argument('--verbose', dest='verbose', action='store_true', help='verbose?')\n",
    "    parser.add_argument('--name', default='real', type=str, help='name of the task')\n",
    "    parser.add_argument('--field_type', default='solenoidal', type=str, help='type of vector field to learn')\n",
    "    parser.add_argument('--baseline', dest='baseline', action='store_true', help='run baseline or experiment?')\n",
    "    parser.add_argument('--use_rk4', dest='use_rk4', action='store_true', help='integrate derivative with RK4')\n",
    "    parser.add_argument('--seed', default=0, type=int, help='random seed')\n",
    "    parser.add_argument('--save_dir', default=EXPERIMENT_DIR, type=str, help='where to save the trained model')\n",
    "    parser.set_defaults(feature=True)\n",
    "\n",
    "    if is_jupyter():\n",
    "        return parser.parse_args([]) \n",
    "    else:\n",
    "        return parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bd26846b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Modification of HNN to the GPU trainable model HNN2\n",
    "\"\"\"\n",
    "class HNN2(torch.nn.Module):\n",
    "    '''Learn arbitrary vector fields that are sums of conservative and solenoidal fields'''\n",
    "    def __init__(self, input_dim, differentiable_model, field_type='solenoidal',\n",
    "                    baseline=False, assume_canonical_coords=True):\n",
    "        super(HNN2, self).__init__()\n",
    "        self.baseline = baseline\n",
    "        self.differentiable_model = differentiable_model\n",
    "        self.assume_canonical_coords = assume_canonical_coords\n",
    "        self.field_type = field_type\n",
    "\n",
    "        # --- Modification 1 for GPU support: register_buffer() is used ---\n",
    "        # self.M = self.permutation_tensor(input_dim) # original HNN code\n",
    "        M_tensor = self.permutation_tensor(input_dim)\n",
    "        self.register_buffer('M_buffer', M_tensor) # M tensor is registered as buffer\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # traditional forward pass\n",
    "        if self.baseline:\n",
    "            return self.differentiable_model(x)\n",
    "\n",
    "        y = self.differentiable_model(x)\n",
    "        assert y.dim() == 2 and y.shape[1] == 2, \"Output tensor should have shape [batch_size, 2]\"\n",
    "        return y.split(1,1)\n",
    "\n",
    "    ''' \n",
    "    Omitted (out of the current interest).\n",
    "       - rk4 function needs to be modified to GPU supporting version \n",
    "       - or torchdiffeq should be used\n",
    "    '''\n",
    "    # def rk4_time_derivative(self, x, dt):\n",
    "    #    return rk4(fun=self.time_derivative, y0=x, t=0, dt=dt)'''\n",
    "\n",
    "    def time_derivative(self, x, t=None, separate_fields=False):\n",
    "        '''NEURAL ODE-STLE VECTOR FIELD'''\n",
    "        if self.baseline:\n",
    "            return self.differentiable_model(x)\n",
    "\n",
    "        '''NEURAL HAMILTONIAN-STLE VECTOR FIELD'''\n",
    "        F1, F2 = self.forward(x) # traditional forward pass\n",
    "\n",
    "        # --- Modification 2 for GPU support: In tensor creation, use x's device ---\n",
    "        conservative_field = torch.zeros_like(x, device=x.device) # device=x.device added\n",
    "        solenoidal_field = torch.zeros_like(x, device=x.device) # device=x.device added\n",
    "\n",
    "        if self.field_type != 'solenoidal':\n",
    "            dF1 = torch.autograd.grad(F1.sum(), x, create_graph=True)[0] # gradients for conservative field\n",
    "            # self.M_buffer   eye \n",
    "            eye = torch.eye(*self.M_buffer.shape, device=self.M_buffer.device)\n",
    "            conservative_field = dF1 @ eye\n",
    "\n",
    "        if self.field_type != 'conservative':\n",
    "            dF2 = torch.autograd.grad(F2.sum(), x, create_graph=True)[0] # gradients for solenoidal field\n",
    "            # Use self.M_buffer\n",
    "            solenoidal_field = dF2 @ self.M_buffer.t()\n",
    "\n",
    "        if separate_fields:\n",
    "            return [conservative_field, solenoidal_field]\n",
    "\n",
    "        return conservative_field + solenoidal_field\n",
    "\n",
    "    def permutation_tensor(self, n):\n",
    "        # Returns torch tensor, and processed at __init__\n",
    "        M = None\n",
    "        if self.assume_canonical_coords:\n",
    "            M = torch.eye(n)\n",
    "            M = torch.cat([M[n//2:], -M[:n//2]])\n",
    "        else:\n",
    "            M = torch.ones(n,n)\n",
    "            M *= 1 - torch.eye(n)\n",
    "            M[::2] *= -1\n",
    "            M[:,::2] *= -1\n",
    "            for i in range(n):\n",
    "                for j in range(i+1, n):\n",
    "                    M[i,j] *= -1\n",
    "        return M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bf72bd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hnn2_model(device):\n",
    "    \"\"\"\n",
    "    Definitions of MLP class at hamiltonian_nn and at dissipative_hnns DIFFER!\n",
    "    We need hamitonian_nn's MLP implementation.\n",
    "    \"\"\"\n",
    "    from hamiltonian_nn.nn_models import MLP\n",
    "\n",
    "    args = get_hnn2_args()\n",
    "    output_dim = args.input_dim\n",
    "    nn_model = MLP(args.input_dim, args.hidden_dim, output_dim, args.nonlinearity)\n",
    "    # Modified HNN class above (HNN2: GPU sersion) is used)\n",
    "    model = HNN2(args.input_dim, differentiable_model=nn_model,\n",
    "              field_type=args.field_type) \n",
    "    model.to(device) \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cd6b43e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hnn2_train():\n",
    "  # adapted to the current hnn-family directory sturctures\n",
    "  from hamiltonian_nn.utils import L2_loss\n",
    "\n",
    "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "  args = get_hnn2_args()\n",
    "  args.verbose = True\n",
    "  \n",
    "  model = get_hnn2_model(device)\n",
    "\n",
    "  # set random seed\n",
    "  torch.manual_seed(args.seed)\n",
    "  np.random.seed(args.seed)\n",
    "\n",
    "  optim = torch.optim.Adam(model.parameters(), args.learn_rate, weight_decay=1e-5)\n",
    "\n",
    "  # arrange data\n",
    "  data = get_dataset('pend-real', args.save_dir)\n",
    "  \n",
    "  # Changed torch.Tensor() of the original HNN to torch.tensor() in HNN2 for GPU support.\n",
    "  x = torch.tensor( data['x'], requires_grad=True, dtype=torch.float32, device=device)\n",
    "  dx = torch.tensor(data['dx'], dtype=torch.float32, device=device) \n",
    "\n",
    "  test_x = torch.tensor( data['test_x'], requires_grad=True, dtype=torch.float32, device=device)\n",
    "  test_dx = torch.tensor(data['test_dx'], dtype=torch.float32, device=device) \n",
    "\n",
    "  # vanilla train loop\n",
    "  stats = {'train_loss': [], 'test_loss': []}\n",
    "  for step in range(args.total_steps+1):\n",
    "\n",
    "    # train step\n",
    "    dx_hat = model.time_derivative(x)\n",
    "    loss = L2_loss(dx, dx_hat)\n",
    "    loss.backward() ; optim.step() ; optim.zero_grad()\n",
    "\n",
    "    # run validation\n",
    "    test_dx_hat = model.time_derivative(test_x)\n",
    "    test_loss = L2_loss(test_dx, test_dx_hat)\n",
    "\n",
    "    # logging\n",
    "    stats['train_loss'].append(loss.item())\n",
    "    stats['test_loss'].append(test_loss.item())\n",
    "    if args.verbose and step % args.print_every == 0:\n",
    "      print(\"step {}, train_loss {:.4e}, test_loss {:.4e}\".format(step, loss.item(), test_loss.item()))\n",
    "\n",
    "  train_dx_hat = model.time_derivative(x)\n",
    "  train_dist = (dx - train_dx_hat)**2\n",
    "  test_dx_hat = model.time_derivative(test_x)\n",
    "  test_dist = (test_dx - test_dx_hat)**2\n",
    "  print('Final train loss {:.4e} +/- {:.4e}\\nFinal test loss {:.4e} +/- {:.4e}'\n",
    "    .format(train_dist.mean().item(), train_dist.std().item()/np.sqrt(train_dist.shape[0]),\n",
    "            test_dist.mean().item(), test_dist.std().item()/np.sqrt(test_dist.shape[0])))\n",
    "\n",
    "  return model, stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8311bc95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data zip filepath: ./experiment-real/invar_datasets.zip\n",
      "step 0, train_loss 7.6955e-01, test_loss 4.5178e-01\n",
      "step 200, train_loss 9.2908e-03, test_loss 6.0311e-03\n",
      "step 400, train_loss 9.1878e-03, test_loss 5.8373e-03\n",
      "step 600, train_loss 9.1703e-03, test_loss 5.8074e-03\n",
      "step 800, train_loss 9.1651e-03, test_loss 5.8024e-03\n",
      "step 1000, train_loss 9.1623e-03, test_loss 5.8015e-03\n",
      "step 1200, train_loss 9.1603e-03, test_loss 5.8013e-03\n",
      "step 1400, train_loss 9.1587e-03, test_loss 5.7976e-03\n",
      "step 1600, train_loss 9.1572e-03, test_loss 5.8020e-03\n",
      "step 1800, train_loss 9.1558e-03, test_loss 5.8023e-03\n",
      "step 2000, train_loss 9.1558e-03, test_loss 5.7980e-03\n",
      "Final train loss 9.1548e-03 +/- 4.9165e-04\n",
      "Final test loss 5.7980e-03 +/- 6.0307e-04\n",
      "Model's weight has successfully been saved: ./experiment-real/weights/hnn2_real.pth\n"
     ]
    }
   ],
   "source": [
    "hnn2_model, stats = hnn2_train()\n",
    "save_dir = EXPERIMENT_DIR + \"/weights\"\n",
    "save_model_weights(hnn2_model, save_dir,\"hnn2_real\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
