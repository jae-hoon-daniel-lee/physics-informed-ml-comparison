{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "954a66c3",
   "metadata": {},
   "source": [
    "# Train HNN family (HNN, DGNet) for 'experiment-2body' problem\n",
    "\n",
    "- Original HNN from hamiltonian_nn by Sam Greydanus\n",
    "\n",
    "- Original DGNet form discrete-autograd by Takashi Matsubara\n",
    "\n",
    "- modified and adapted by Jae Hoon (Daniel) Lee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8316a2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.set_default_dtype(torch.float32)\n",
    "import argparse\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "EXPERIMENT_DIR = './experiment-2body'\n",
    "sys.path.append(EXPERIMENT_DIR)\n",
    "\n",
    "from data import get_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b524b6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_jupyter():\n",
    "    return 'ipykernel' in sys.modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52c000f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_weights(model, directory, filename):\n",
    "    try:\n",
    "        if not filename.lower().endswith(('.pth', '.pt', '.tar')):\n",
    "            filename += '.pth'\n",
    "        \n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "\n",
    "        full_path = os.path.join(directory, filename)\n",
    "        torch.save(model.state_dict(), full_path)\n",
    "        \n",
    "        print(f\"Model weights have successfully been saved: {full_path}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred while saving the model: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b8e80a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_weights(model, filepath, device='cpu'):\n",
    "    if not os.path.exists(filepath):\n",
    "        print(f\"Error: file not found: {filepath}\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        state_dict = torch.load(filepath, map_location=torch.device(device))\n",
    "        model.load_state_dict(state_dict)\n",
    "        model.to(device)\n",
    "\n",
    "        print(f\"Model's weights have been successfully loaded: {filepath} (Device: {device})\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred while loading the model: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7042e03",
   "metadata": {},
   "source": [
    "## DGNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c24f456",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b59a1d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dgnet_args():\n",
    "    parser = argparse.ArgumentParser(description=None)\n",
    "    parser.add_argument('--noretry', dest='noretry', action='store_true', help='not do a finished trial.')\n",
    "    # network, experiments\n",
    "    parser.add_argument('--input_dim', default=2 * 4, type=int, help='dimensionality of input tensor')\n",
    "    parser.add_argument('--hidden_dim', default=200, type=int, help='hidden dimension of mlp')\n",
    "    parser.add_argument('--learn_rate', default=1e-3, type=float, help='learning rate')\n",
    "    parser.add_argument('--batch_size', default=200, type=int, help='batch_size')\n",
    "    parser.add_argument('--nonlinearity', default='tanh', type=str, help='neural net nonlinearity')\n",
    "    parser.add_argument('--total_steps', default=10000, type=int, help='number of gradient steps')\n",
    "    parser.add_argument('--input_noise', default=0.0, type=int, help='std of noise added to inputs')\n",
    "    # display\n",
    "    parser.add_argument('--print_every', default=200, type=int, help='number of gradient steps between prints')\n",
    "    parser.add_argument('--verbose', dest='verbose', action='store_true', help='verbose?')\n",
    "    parser.add_argument('--name', default='2body', type=str, help='only one option right now')\n",
    "    parser.add_argument('--seed', default=0, type=int, help='random seed')\n",
    "    parser.add_argument('--save_dir', default=EXPERIMENT_DIR, type=str, help='where to save the trained model')\n",
    "    # model\n",
    "    parser.add_argument('--model', default='hnn', type=str, help='used model.')\n",
    "    parser.add_argument('--solver', default='dg', type=str, help='used solver.')\n",
    "    parser.add_argument('--friction', default=False, action=\"store_true\", help='use friction parameter')\n",
    "    parser.set_defaults(feature=True)\n",
    "\n",
    "    if is_jupyter():\n",
    "        return parser.parse_args([]) \n",
    "    else:\n",
    "        return parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb6215eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dgnet_model(device='cpu'):\n",
    "\n",
    "    args = get_dgnet_args()\n",
    "    model = dgnet.DGNet(args.input_dim, args.hidden_dim,\n",
    "                        nonlinearity=args.nonlinearity, friction=args.friction, model=args.model, solver=args.solver)\n",
    "    model = model.to(device)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82f946df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dgnet_train(args):\n",
    "    ''' import from the current directory structure '''\n",
    "    from hamiltonian_nn.utils import L2_loss\n",
    "\n",
    "    device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "    dtype = torch.get_default_dtype()\n",
    "    torch.set_grad_enabled(False)\n",
    "\n",
    "    # set random seed\n",
    "    torch.manual_seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "\n",
    "    # init model and optimizer\n",
    "    model = dgnet.DGNet(args.input_dim, args.hidden_dim,\n",
    "                        nonlinearity=args.nonlinearity, model=args.model, solver=args.solver)\n",
    "    model = model.to(device)\n",
    "    optim = torch.optim.Adam(model.parameters(), args.learn_rate, weight_decay=0)\n",
    "\n",
    "    # arrange data\n",
    "    t_span = 25\n",
    "    length = 500\n",
    "    dt = t_span / (length - 1)\n",
    "    data = get_dataset(args.name, args.save_dir, verbose=True, timesteps=length, t_span=[0, t_span])\n",
    "    train_x = torch.tensor(data['coords'], requires_grad=True, device=device, dtype=dtype)\n",
    "    test_x = torch.tensor(data['test_coords'], requires_grad=True, device=device, dtype=dtype)\n",
    "    train_dxdt = torch.tensor(data['dcoords'], device=device, dtype=dtype)\n",
    "    test_dxdt = torch.tensor(data['test_dcoords'], device=device, dtype=dtype)\n",
    "    x_reshaped = train_x.view(-1, length, args.input_dim)\n",
    "    x1 = x_reshaped[:, :-1].contiguous().view(-1, args.input_dim)\n",
    "    x2 = x_reshaped[:, 1:].contiguous().view(-1, args.input_dim)\n",
    "    dxdt = ((x2 - x1) / dt).detach()\n",
    "\n",
    "    # vanilla train loop\n",
    "    stats = {'train_loss': [], 'test_loss': []}\n",
    "    for step in range(args.total_steps + 1):\n",
    "        with torch.enable_grad():\n",
    "            # train step\n",
    "            ixs = torch.randperm(x1.shape[0])[:args.batch_size]\n",
    "            dxdt_hat = model.discrete_time_derivative(x1[ixs], dt=dt, x2=x2[ixs])\n",
    "            dxdt_hat += args.input_noise * torch.randn_like(x1[ixs])  # add noise, maybe\n",
    "            loss = L2_loss(dxdt[ixs], dxdt_hat)\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        optim.zero_grad()\n",
    "\n",
    "        # run test data\n",
    "        test_ixs = torch.randperm(test_x.shape[0], device=device)[:args.batch_size]\n",
    "        test_dxdt_hat = model.time_derivative(test_x[test_ixs])\n",
    "        test_dxdt_hat += args.input_noise * torch.randn_like(test_x[test_ixs])  # add noise, maybe\n",
    "        test_loss = L2_loss(test_dxdt[test_ixs], test_dxdt_hat)\n",
    "\n",
    "        # logging\n",
    "        stats['train_loss'].append(loss.item())\n",
    "        stats['test_loss'].append(test_loss.item())\n",
    "        if args.verbose and step % args.print_every == 0:\n",
    "            print(\"step {}, train_loss {:.4e}, test_loss {:.4e}\"\n",
    "                  .format(step, loss.item(), test_loss.item()))\n",
    "            if args.friction:\n",
    "                print(\"friction g =\", model.g.detach().cpu().numpy())\n",
    "\n",
    "    train_dxdt_hat = model.time_derivative(train_x)\n",
    "    train_dist = (train_dxdt - train_dxdt_hat)**2\n",
    "    test_dxdt_hat = model.time_derivative(test_x)\n",
    "    test_dist = (test_dxdt - test_dxdt_hat)**2\n",
    "    print('Final train loss {:.4e}\\nFinal test loss {:.4e}'\n",
    "          .format(train_dist.mean().item(), test_dist.mean().item()))\n",
    "    stats['final_train_loss'] = train_dist.mean().item()\n",
    "    stats['final_test_loss'] = test_dist.mean().item()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d63f400",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dgnet_main():\n",
    "    args = get_dgnet_args()\n",
    "\n",
    "    # Explicilty set args.verbose attribute to True\n",
    "    args.verbose = True\n",
    "    args.print_every = 200\n",
    "\n",
    "    # save\n",
    "    os.makedirs(args.save_dir + '/results') if not os.path.exists(args.save_dir + '/results') else None\n",
    "    label = ''\n",
    "    label = label + '-{}-{}'.format(args.model, args.solver)\n",
    "    label = label + '-friction' if args.friction else label\n",
    "    label = label + '-seed{}'.format(args.seed)\n",
    "    name = args.name\n",
    "    result_path = '{}/results/dg-{}{}'.format(args.save_dir, name, label)\n",
    "    path_tar = '{}.tar'.format(result_path)\n",
    "    path_pkl = '{}.pkl'.format(result_path)\n",
    "    path_txt = '{}.txt'.format(result_path)\n",
    "\n",
    "    if os.path.exists(path_txt):\n",
    "        if args.noretry:\n",
    "            exit()\n",
    "        else:\n",
    "            os.remove(path_txt)\n",
    "\n",
    "    model = dgnet_train(args)\n",
    "    torch.save(model.state_dict(), path_tar)\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d043c3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded data from ./experiment-2body/2body-orbits-dataset.pkl\n",
      "step 0, train_loss 6.3007e-02, test_loss 5.0414e-02\n",
      "step 200, train_loss 1.1355e-03, test_loss 8.8807e-04\n",
      "step 400, train_loss 3.4787e-04, test_loss 2.7645e-04\n",
      "step 600, train_loss 3.5661e-04, test_loss 2.1784e-04\n",
      "step 800, train_loss 1.7680e-04, test_loss 1.8877e-04\n",
      "step 1000, train_loss 1.2389e-04, test_loss 1.3109e-04\n",
      "step 1200, train_loss 8.3940e-05, test_loss 7.5953e-05\n",
      "step 1400, train_loss 4.0289e-05, test_loss 4.8356e-05\n",
      "step 1600, train_loss 3.3955e-05, test_loss 3.5355e-05\n",
      "step 1800, train_loss 4.0630e-05, test_loss 2.8098e-05\n",
      "step 2000, train_loss 3.5671e-05, test_loss 2.6532e-05\n",
      "step 2200, train_loss 2.7439e-05, test_loss 2.4724e-05\n",
      "step 2400, train_loss 7.6412e-05, test_loss 2.2852e-05\n",
      "step 2600, train_loss 1.7805e-05, test_loss 1.8627e-05\n",
      "step 2800, train_loss 1.8993e-05, test_loss 2.8659e-05\n",
      "step 3000, train_loss 1.9339e-05, test_loss 1.6054e-05\n",
      "step 3200, train_loss 1.7628e-05, test_loss 2.3954e-05\n",
      "step 3400, train_loss 1.8237e-05, test_loss 1.9065e-05\n",
      "step 3600, train_loss 1.4922e-05, test_loss 1.4757e-05\n",
      "step 3800, train_loss 1.7678e-05, test_loss 1.5748e-05\n",
      "step 4000, train_loss 1.2863e-05, test_loss 1.1100e-05\n",
      "step 4200, train_loss 3.6399e-05, test_loss 1.2962e-05\n",
      "step 4400, train_loss 1.7115e-05, test_loss 1.7823e-05\n",
      "step 4600, train_loss 2.0637e-05, test_loss 1.5949e-05\n",
      "step 4800, train_loss 1.5394e-05, test_loss 1.9762e-05\n",
      "step 5000, train_loss 3.0948e-05, test_loss 3.5069e-05\n",
      "step 5200, train_loss 2.0626e-05, test_loss 1.2119e-05\n",
      "step 5400, train_loss 1.8019e-05, test_loss 1.1548e-05\n",
      "step 5600, train_loss 1.3328e-05, test_loss 1.6111e-05\n",
      "step 5800, train_loss 1.0082e-05, test_loss 1.4974e-05\n",
      "step 6000, train_loss 1.4634e-05, test_loss 1.3964e-05\n",
      "step 6200, train_loss 1.1313e-05, test_loss 1.1640e-05\n",
      "step 6400, train_loss 1.6370e-05, test_loss 1.1679e-05\n",
      "step 6600, train_loss 9.0679e-06, test_loss 8.7447e-06\n",
      "step 6800, train_loss 6.1986e-06, test_loss 6.7430e-06\n",
      "step 7000, train_loss 1.0844e-05, test_loss 9.6484e-06\n",
      "step 7200, train_loss 1.1370e-05, test_loss 6.8701e-06\n",
      "step 7400, train_loss 9.8008e-06, test_loss 1.8374e-05\n",
      "step 7600, train_loss 1.7153e-05, test_loss 9.4160e-06\n",
      "step 7800, train_loss 1.1683e-05, test_loss 1.2904e-05\n",
      "step 8000, train_loss 7.1596e-06, test_loss 5.0724e-06\n",
      "step 8200, train_loss 5.5969e-06, test_loss 5.4536e-06\n",
      "step 8400, train_loss 7.5612e-06, test_loss 8.6088e-06\n",
      "step 8600, train_loss 9.6268e-06, test_loss 9.5366e-06\n",
      "step 8800, train_loss 7.8094e-06, test_loss 1.1414e-05\n",
      "step 9000, train_loss 9.9096e-06, test_loss 1.1552e-05\n",
      "step 9200, train_loss 6.2638e-06, test_loss 6.9712e-06\n",
      "step 9400, train_loss 1.3130e-05, test_loss 1.4021e-05\n",
      "step 9600, train_loss 4.2275e-06, test_loss 4.7634e-06\n",
      "step 9800, train_loss 6.0219e-06, test_loss 7.4215e-06\n",
      "step 10000, train_loss 7.0871e-06, test_loss 6.1036e-06\n",
      "Final train loss 6.5104e-06\n",
      "Final test loss 6.1978e-06\n"
     ]
    }
   ],
   "source": [
    "dgnet_model = train_dgnet_main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dcfc2a4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model weights have successfully been saved: ./experiment-2body/weights/dgnet_2body.pth\n"
     ]
    }
   ],
   "source": [
    "save_dir = EXPERIMENT_DIR + \"/weights\"\n",
    "save_model_weights(dgnet_model, save_dir,\"dgnet_2body\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7462353",
   "metadata": {},
   "source": [
    "## HNN \n",
    "(Hamiltonian Neural Netowork: modified version for GPU support: named as HNN2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b7c5c3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hnn2_args():\n",
    "    parser = argparse.ArgumentParser(description=None)\n",
    "    parser.add_argument('--input_dim', default=2*4, type=int, help='dimensionality of input tensor')\n",
    "    parser.add_argument('--hidden_dim', default=200, type=int, help='hidden dimension of mlp')\n",
    "    parser.add_argument('--learn_rate', default=1e-3, type=float, help='learning rate')\n",
    "    parser.add_argument('--batch_size', default=200, type=int, help='batch_size')\n",
    "    parser.add_argument('--input_noise', default=0.0, type=int, help='std of noise added to inputs')\n",
    "    parser.add_argument('--nonlinearity', default='tanh', type=str, help='neural net nonlinearity')\n",
    "    parser.add_argument('--total_steps', default=10000, type=int, help='number of gradient steps')\n",
    "    parser.add_argument('--print_every', default=200, type=int, help='number of gradient steps between prints')\n",
    "    parser.add_argument('--name', default='2body', type=str, help='only one option right now')\n",
    "    parser.add_argument('--baseline', dest='baseline', action='store_true', help='run baseline or experiment?')\n",
    "    parser.add_argument('--verbose', dest='verbose', action='store_true', help='verbose?')\n",
    "    parser.add_argument('--field_type', default='solenoidal', type=str, help='type of vector field to learn')\n",
    "    parser.add_argument('--seed', default=0, type=int, help='random seed')\n",
    "    parser.add_argument('--save_dir', default=EXPERIMENT_DIR, type=str, help='where to save the trained model')\n",
    "    parser.set_defaults(feature=True)\n",
    "\n",
    "    if is_jupyter():\n",
    "        return parser.parse_args([]) \n",
    "    else:\n",
    "        return parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bae78afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HNN2(torch.nn.Module):\n",
    "    '''Learn arbitrary vector fields that are sums of conservative and solenoidal fields'''\n",
    "    def __init__(self, input_dim, differentiable_model, field_type='solenoidal',\n",
    "                    baseline=False, assume_canonical_coords=True):\n",
    "        super(HNN2, self).__init__()\n",
    "        self.baseline = baseline\n",
    "        self.differentiable_model = differentiable_model\n",
    "        self.assume_canonical_coords = assume_canonical_coords\n",
    "        self.field_type = field_type\n",
    "\n",
    "        # --- modification 1 for GPU support: register_buffer is used ---\n",
    "        # self.M = self.permutation_tensor(input_dim) # original HNN code by Sam Greydanus\n",
    "        M_tensor = self.permutation_tensor(input_dim)\n",
    "        self.register_buffer('M_buffer', M_tensor) # register M tensor as buffer\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # traditional forward pass\n",
    "        if self.baseline:\n",
    "            return self.differentiable_model(x)\n",
    "\n",
    "        y = self.differentiable_model(x)\n",
    "        assert y.dim() == 2 and y.shape[1] == 2, \"Output tensor should have shape [batch_size, 2]\"\n",
    "        return y.split(1,1)\n",
    "\n",
    "    '''def rk4_time_derivative(self, x, dt):\n",
    "        # need to be modified to support GPU in rk4 or to use torchdiffeq\n",
    "        return rk4(fun=self.time_derivative, y0=x, t=0, dt=dt)'''\n",
    "\n",
    "    def time_derivative(self, x, t=None, separate_fields=False):\n",
    "        '''NEURAL ODE-STLE VECTOR FIELD'''\n",
    "        if self.baseline:\n",
    "            return self.differentiable_model(x)\n",
    "\n",
    "        '''NEURAL HAMILTONIAN-STLE VECTOR FIELD'''\n",
    "        F1, F2 = self.forward(x) # traditional forward pass\n",
    "\n",
    "        # --- modification 2 for GPU support: use x's device when creating tensors ---\n",
    "        conservative_field = torch.zeros_like(x, device=x.device) # device=x.device\n",
    "        solenoidal_field = torch.zeros_like(x, device=x.device) # device=x.device\n",
    "\n",
    "        if self.field_type != 'solenoidal':\n",
    "            dF1 = torch.autograd.grad(F1.sum(), x, create_graph=True)[0] # gradients for conservative field\n",
    "            # create eye on the same device where self.M_buffer lives\n",
    "            eye = torch.eye(*self.M_buffer.shape, device=self.M_buffer.device)\n",
    "            conservative_field = dF1 @ eye\n",
    "\n",
    "        if self.field_type != 'conservative':\n",
    "            dF2 = torch.autograd.grad(F2.sum(), x, create_graph=True)[0] # gradients for solenoidal field\n",
    "            # self.M_buffer is used\n",
    "            solenoidal_field = dF2 @ self.M_buffer.t()\n",
    "\n",
    "        if separate_fields:\n",
    "            return [conservative_field, solenoidal_field]\n",
    "\n",
    "        return conservative_field + solenoidal_field\n",
    "\n",
    "    def permutation_tensor(self, n):\n",
    "        # returns a torch tensor and processed at __init__\n",
    "        M = None\n",
    "        if self.assume_canonical_coords:\n",
    "            M = torch.eye(n)\n",
    "            M = torch.cat([M[n//2:], -M[:n//2]])\n",
    "        else:\n",
    "            M = torch.ones(n,n)\n",
    "            M *= 1 - torch.eye(n)\n",
    "            M[::2] *= -1\n",
    "            M[:,::2] *= -1\n",
    "            for i in range(n):\n",
    "                for j in range(i+1, n):\n",
    "                    M[i,j] *= -1\n",
    "        return M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e61124a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hnn2_model(device):\n",
    "    from hamiltonian_nn.nn_models import MLP\n",
    "    args = get_hnn2_args()\n",
    "    output_dim = 2\n",
    "    nn_model = MLP(args.input_dim, args.hidden_dim, output_dim, args.nonlinearity)\n",
    "    # The modified HNN class above (HNN2: GPU version) is used.\n",
    "    model = HNN2(args.input_dim, differentiable_model=nn_model,\n",
    "              field_type=args.field_type) \n",
    "    model.to(device) \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "27997dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hnn2_train():\n",
    "  # adapted to the current hnn-family directory sturctures\n",
    "  from hamiltonian_nn.utils import L2_loss\n",
    "\n",
    "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "  torch.set_grad_enabled(True)\n",
    "  \n",
    "  args = get_hnn2_args()\n",
    "  args.verbose = True\n",
    "\n",
    "  # set random seed\n",
    "  torch.manual_seed(args.seed)\n",
    "  np.random.seed(args.seed)\n",
    "  \n",
    "  model = get_hnn2_model(device)\n",
    "\n",
    "  optim = torch.optim.Adam(model.parameters(), args.learn_rate, weight_decay=0)\n",
    "\n",
    "  # arrange data\n",
    "  data = get_dataset(args.name, args.save_dir, verbose=True)\n",
    "\n",
    "   # torch.Tensor() in the original HNN by Sam Greydanus, \n",
    "   # has been replaced by torch.tensor() with device here for GPU.\n",
    "  x = torch.tensor( data['coords'], requires_grad=True, dtype=torch.float32, device=device)\n",
    "  test_x = torch.tensor( data['test_coords'], requires_grad=True, dtype=torch.float32, device=device)\n",
    "  dxdt = torch.tensor(data['dcoords'], dtype=torch.float32, device=device) \n",
    "  test_dxdt = torch.tensor(data['test_dcoords'], dtype=torch.float32, device=device) \n",
    "\n",
    "  # vanilla train loop\n",
    "  stats = {'train_loss': [], 'test_loss': []}\n",
    "  for step in range(args.total_steps+1):\n",
    "\n",
    "    # train step\n",
    "    ixs = torch.randperm(x.shape[0])[:args.batch_size] # exists in the original version of HNN \n",
    "    ixs_gpu = ixs.to(device) # Now, we should assign it to GPU.\n",
    "    dxdt_hat = model.time_derivative(x[ixs_gpu]) # calculated in the GPU context, now.\n",
    "    noise = args.input_noise * torch.randn(*x[ixs].shape) # add noise, maybe\n",
    "    dxdt_hat += noise.to(device) # This also should be modified to be casted into GPU.\n",
    "    loss = L2_loss(dxdt[ixs_gpu], dxdt_hat)\n",
    "    loss.backward()\n",
    "    grad = torch.cat([p.grad.flatten() for p in model.parameters()]).clone()\n",
    "    optim.step() ; optim.zero_grad()\n",
    "\n",
    "    # run test data\n",
    "    test_ixs = torch.randperm(test_x.shape[0])[:args.batch_size]\n",
    "    test_ixs_gpu = test_ixs.to(device) # assigned to GPU\n",
    "    test_dxdt_hat = model.time_derivative(test_x[test_ixs_gpu]) # calculated in the GPU context, now.\n",
    "    noise = args.input_noise * torch.randn(*test_x[test_ixs].shape) # add noise, maybe\n",
    "    test_dxdt_hat += noise.to(device) # This also should be modified to be casted into GPU.\n",
    "    test_loss = L2_loss(test_dxdt[test_ixs_gpu], test_dxdt_hat)\n",
    "\n",
    "    # logging\n",
    "    stats['train_loss'].append(loss.item())\n",
    "    stats['test_loss'].append(test_loss.item())\n",
    "    if args.verbose and step % args.print_every == 0:\n",
    "      print(\"step {}, train_loss {:.4e}, test_loss {:.4e}, grad norm {:.4e}, grad std {:.4e}\"\n",
    "          .format(step, loss.item(), test_loss.item(), grad@grad, grad.std()))\n",
    "\n",
    "  train_dxdt_hat = model.time_derivative(x)\n",
    "  train_dist = (dxdt - train_dxdt_hat)**2\n",
    "  test_dxdt_hat = model.time_derivative(test_x)\n",
    "  test_dist = (test_dxdt - test_dxdt_hat)**2\n",
    "  print('Final train loss {:.4e} +/- {:.4e}\\nFinal test loss {:.4e} +/- {:.4e}'\n",
    "    .format(train_dist.mean().item(), train_dist.std().item()/np.sqrt(train_dist.shape[0]),\n",
    "            test_dist.mean().item(), test_dist.std().item()/np.sqrt(test_dist.shape[0])))\n",
    "  return model, stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4fb604d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded data from ./experiment-2body/2body-orbits-dataset.pkl\n",
      "step 0, train_loss 5.6959e-02, test_loss 5.0372e-02, grad norm 9.6848e-03, grad std 4.7793e-04\n",
      "step 200, train_loss 4.7525e-04, test_loss 3.9000e-04, grad norm 6.9597e-05, grad std 4.0514e-05\n",
      "step 400, train_loss 2.3650e-04, test_loss 1.6555e-04, grad norm 5.8871e-05, grad std 3.7262e-05\n",
      "step 600, train_loss 2.4006e-04, test_loss 9.9555e-05, grad norm 1.7016e-05, grad std 2.0032e-05\n",
      "step 800, train_loss 6.8406e-05, test_loss 6.3686e-05, grad norm 3.1591e-05, grad std 2.7296e-05\n",
      "step 1000, train_loss 5.4871e-05, test_loss 4.5237e-05, grad norm 2.8903e-05, grad std 2.6109e-05\n",
      "step 1200, train_loss 4.4448e-05, test_loss 3.4072e-05, grad norm 3.5755e-05, grad std 2.9040e-05\n",
      "step 1400, train_loss 2.4692e-05, test_loss 4.0050e-05, grad norm 6.5014e-06, grad std 1.2383e-05\n",
      "step 1600, train_loss 3.0835e-05, test_loss 1.9492e-05, grad norm 2.5659e-05, grad std 2.4600e-05\n",
      "step 1800, train_loss 2.4295e-05, test_loss 3.1739e-05, grad norm 3.8358e-05, grad std 3.0078e-05\n",
      "step 2000, train_loss 3.8154e-05, test_loss 1.6875e-05, grad norm 4.5089e-05, grad std 3.2610e-05\n",
      "step 2200, train_loss 1.8697e-05, test_loss 1.4794e-05, grad norm 2.5606e-05, grad std 2.4575e-05\n",
      "step 2400, train_loss 1.3212e-05, test_loss 1.5013e-05, grad norm 5.9741e-06, grad std 1.1870e-05\n",
      "step 2600, train_loss 1.1528e-05, test_loss 1.2517e-05, grad norm 7.7460e-06, grad std 1.3516e-05\n",
      "step 2800, train_loss 1.2472e-05, test_loss 1.3385e-05, grad norm 7.2629e-06, grad std 1.3088e-05\n",
      "step 3000, train_loss 1.1129e-05, test_loss 8.6381e-06, grad norm 1.3378e-05, grad std 1.7763e-05\n",
      "step 3200, train_loss 1.1014e-05, test_loss 8.8721e-06, grad norm 1.3349e-05, grad std 1.7744e-05\n",
      "step 3400, train_loss 1.5691e-05, test_loss 1.1080e-05, grad norm 2.1247e-05, grad std 2.2386e-05\n",
      "step 3600, train_loss 1.5677e-05, test_loss 1.2367e-05, grad norm 2.6414e-05, grad std 2.4959e-05\n",
      "step 3800, train_loss 2.0890e-05, test_loss 6.6139e-06, grad norm 3.7950e-06, grad std 9.4608e-06\n",
      "step 4000, train_loss 5.1534e-06, test_loss 8.0710e-06, grad norm 1.9557e-06, grad std 6.7915e-06\n",
      "step 4200, train_loss 9.2973e-06, test_loss 1.3597e-05, grad norm 1.2912e-05, grad std 1.7450e-05\n",
      "step 4400, train_loss 1.0796e-05, test_loss 1.2747e-05, grad norm 1.1293e-05, grad std 1.6320e-05\n",
      "step 4600, train_loss 5.4968e-06, test_loss 6.1082e-06, grad norm 2.8438e-06, grad std 8.1897e-06\n",
      "step 4800, train_loss 8.2553e-06, test_loss 8.6586e-06, grad norm 1.2052e-05, grad std 1.6860e-05\n",
      "step 5000, train_loss 8.9369e-06, test_loss 1.1720e-05, grad norm 1.2110e-05, grad std 1.6899e-05\n",
      "step 5200, train_loss 8.4477e-06, test_loss 7.4851e-06, grad norm 1.2031e-05, grad std 1.6845e-05\n",
      "step 5400, train_loss 8.9830e-06, test_loss 6.5933e-06, grad norm 5.8725e-06, grad std 1.1769e-05\n",
      "step 5600, train_loss 6.3361e-06, test_loss 4.2318e-06, grad norm 1.0640e-05, grad std 1.5841e-05\n",
      "step 5800, train_loss 1.4983e-05, test_loss 1.9461e-05, grad norm 3.7114e-05, grad std 2.9586e-05\n",
      "step 6000, train_loss 1.2228e-05, test_loss 1.4205e-05, grad norm 3.3177e-05, grad std 2.7973e-05\n",
      "step 6200, train_loss 1.0146e-05, test_loss 8.2002e-06, grad norm 1.8814e-05, grad std 2.1065e-05\n",
      "step 6400, train_loss 3.5650e-06, test_loss 3.8596e-06, grad norm 1.9059e-06, grad std 6.7041e-06\n",
      "step 6600, train_loss 1.0980e-05, test_loss 8.8862e-06, grad norm 1.8986e-05, grad std 2.1161e-05\n",
      "step 6800, train_loss 3.6510e-06, test_loss 5.0915e-06, grad norm 2.0683e-06, grad std 6.9844e-06\n",
      "step 7000, train_loss 9.4206e-06, test_loss 1.5823e-05, grad norm 9.2242e-06, grad std 1.4750e-05\n",
      "step 7200, train_loss 4.3551e-06, test_loss 3.7785e-06, grad norm 2.4861e-06, grad std 7.6572e-06\n",
      "step 7400, train_loss 4.2196e-06, test_loss 2.9740e-06, grad norm 5.3992e-06, grad std 1.1284e-05\n",
      "step 7600, train_loss 1.7204e-05, test_loss 9.6173e-06, grad norm 3.6180e-05, grad std 2.9211e-05\n",
      "step 7800, train_loss 1.0020e-05, test_loss 7.8940e-06, grad norm 8.7338e-06, grad std 1.4352e-05\n",
      "step 8000, train_loss 2.6729e-06, test_loss 3.1113e-06, grad norm 2.1083e-06, grad std 7.0516e-06\n",
      "step 8200, train_loss 4.8221e-06, test_loss 4.4257e-06, grad norm 7.8227e-06, grad std 1.3583e-05\n",
      "step 8400, train_loss 2.4985e-06, test_loss 3.3115e-06, grad norm 2.2194e-06, grad std 7.2350e-06\n",
      "step 8600, train_loss 3.9888e-06, test_loss 2.7042e-06, grad norm 4.9788e-06, grad std 1.0836e-05\n",
      "step 8800, train_loss 3.4507e-06, test_loss 4.8909e-06, grad norm 4.4291e-06, grad std 1.0220e-05\n",
      "step 9000, train_loss 1.3329e-05, test_loss 1.3786e-05, grad norm 1.7226e-05, grad std 2.0156e-05\n",
      "step 9200, train_loss 3.0357e-06, test_loss 3.4360e-06, grad norm 3.9714e-06, grad std 9.6782e-06\n",
      "step 9400, train_loss 3.4338e-06, test_loss 4.6615e-06, grad norm 5.5750e-06, grad std 1.1467e-05\n",
      "step 9600, train_loss 3.3412e-06, test_loss 3.8073e-06, grad norm 3.6476e-06, grad std 9.2752e-06\n",
      "step 9800, train_loss 9.2173e-06, test_loss 7.3833e-06, grad norm 1.8311e-05, grad std 2.0781e-05\n",
      "step 10000, train_loss 3.7637e-06, test_loss 4.2799e-06, grad norm 4.7117e-06, grad std 1.0542e-05\n",
      "Final train loss 4.5852e-06 +/- 3.4427e-08\n",
      "Final test loss 4.3199e-06 +/- 1.7019e-08\n"
     ]
    }
   ],
   "source": [
    "hnn2_model, stats=hnn2_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "38057256",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model weights have successfully been saved: ./experiment-2body/weights/hnn2_2body.pth\n"
     ]
    }
   ],
   "source": [
    "save_dir = EXPERIMENT_DIR + \"/weights\"\n",
    "save_model_weights(hnn2_model, save_dir,\"hnn2_2body\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
